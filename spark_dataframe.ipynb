{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"test\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"fhvhv/2021/01/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-02 10:25:05|2021-01-02 10:43:18|         188|          35|\n",
      "|2021-01-01 19:13:07|2021-01-01 19:36:13|          18|         208|\n",
      "|2021-01-02 15:58:09|2021-01-02 16:08:09|         249|         144|\n",
      "|2021-01-02 05:32:21|2021-01-02 05:43:24|          81|          32|\n",
      "|2021-01-01 02:57:42|2021-01-01 03:14:24|         232|          48|\n",
      "|2021-01-01 18:25:54|2021-01-01 18:35:59|         244|         244|\n",
      "|2021-01-01 05:06:53|2021-01-01 05:38:58|         229|          38|\n",
      "|2021-01-02 08:52:47|2021-01-02 08:57:52|         107|         162|\n",
      "|2021-01-02 15:12:35|2021-01-02 15:22:56|          78|         242|\n",
      "|2021-01-02 23:36:12|2021-01-03 00:19:14|         138|         265|\n",
      "|2021-01-02 00:30:11|2021-01-02 00:34:32|          48|         239|\n",
      "|2021-01-03 02:19:44|2021-01-03 02:44:44|         254|         224|\n",
      "|2021-01-01 02:52:29|2021-01-01 03:11:32|          50|         145|\n",
      "|2021-01-01 08:30:22|2021-01-01 08:39:49|          80|         195|\n",
      "|2021-01-02 19:30:13|2021-01-02 19:53:06|         159|          81|\n",
      "|2021-01-02 18:32:32|2021-01-02 18:52:49|         144|         246|\n",
      "|2021-01-01 09:39:25|2021-01-01 09:46:30|          74|          42|\n",
      "|2021-01-02 23:35:29|2021-01-02 23:45:50|         189|         188|\n",
      "|2021-01-03 10:58:02|2021-01-03 11:14:06|         238|         152|\n",
      "|2021-01-01 10:39:11|2021-01-01 10:51:55|         263|         151|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID')\\\n",
    "    .filter(df.hvfhs_license_num == \"HV0003\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: User Define Function\n",
    "\n",
    "This one can be an alternative way to aggregate data like we did in SQL but using Spark UDF instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-01-02|  2021-01-02|         188|          35|\n",
      "| 2021-01-01|  2021-01-01|          18|         208|\n",
      "| 2021-01-02|  2021-01-02|         249|         144|\n",
      "| 2021-01-02|  2021-01-02|          81|          32|\n",
      "| 2021-01-01|  2021-01-01|         232|          48|\n",
      "| 2021-01-01|  2021-01-01|         244|         244|\n",
      "| 2021-01-01|  2021-01-01|         229|          38|\n",
      "| 2021-01-02|  2021-01-02|         107|         162|\n",
      "| 2021-01-01|  2021-01-01|          76|          71|\n",
      "| 2021-01-02|  2021-01-02|          78|         242|\n",
      "| 2021-01-02|  2021-01-03|         138|         265|\n",
      "| 2021-01-02|  2021-01-02|          48|         239|\n",
      "| 2021-01-03|  2021-01-03|         254|         224|\n",
      "| 2021-01-02|  2021-01-02|         138|         170|\n",
      "| 2021-01-01|  2021-01-01|          50|         145|\n",
      "| 2021-01-01|  2021-01-01|          80|         195|\n",
      "| 2021-01-02|  2021-01-02|         159|          81|\n",
      "| 2021-01-02|  2021-01-02|         144|         246|\n",
      "| 2021-01-01|  2021-01-01|          74|          42|\n",
      "| 2021-01-02|  2021-01-02|         189|         188|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime))\\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime))\\\n",
    "    .select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our own function\n",
    "\n",
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'\n",
    "    \n",
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df\\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime))\\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime))\\\n",
    "    .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num))\\\n",
    "    .select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+------------+------------+\n",
      "|base_id|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "|  e/a39| 2021-01-02|  2021-01-02|         188|          35|\n",
      "|  e/acc| 2021-01-01|  2021-01-01|          18|         208|\n",
      "|  s/b13| 2021-01-02|  2021-01-02|         249|         144|\n",
      "|  s/b36| 2021-01-02|  2021-01-02|          81|          32|\n",
      "|  e/acc| 2021-01-01|  2021-01-01|         232|          48|\n",
      "|  e/acc| 2021-01-01|  2021-01-01|         244|         244|\n",
      "|  e/b3e| 2021-01-01|  2021-01-01|         229|          38|\n",
      "|  e/b3b| 2021-01-02|  2021-01-02|         107|         162|\n",
      "|  e/9ce| 2021-01-01|  2021-01-01|          76|          71|\n",
      "|  e/b30| 2021-01-02|  2021-01-02|          78|         242|\n",
      "|  e/b48| 2021-01-02|  2021-01-03|         138|         265|\n",
      "|  e/b38| 2021-01-02|  2021-01-02|          48|         239|\n",
      "|  e/b3f| 2021-01-03|  2021-01-03|         254|         224|\n",
      "|  e/9ce| 2021-01-02|  2021-01-02|         138|         170|\n",
      "|  e/acc| 2021-01-01|  2021-01-01|          50|         145|\n",
      "|  e/b47| 2021-01-01|  2021-01-01|          80|         195|\n",
      "|  e/b38| 2021-01-02|  2021-01-02|         159|          81|\n",
      "|  e/b38| 2021-01-02|  2021-01-02|         144|         246|\n",
      "|  e/acc| 2021-01-01|  2021-01-01|          74|          42|\n",
      "|  e/b33| 2021-01-02|  2021-01-02|         189|         188|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|base_id| count|\n",
      "+-------+------+\n",
      "|  e/9d0| 44231|\n",
      "|  e/b3b|735450|\n",
      "|  s/b13|200129|\n",
      "|  e/a7a|321599|\n",
      "|  s/b3d|208986|\n",
      "|  s/b44|257674|\n",
      "|  e/b3e|312013|\n",
      "|  e/b48|177542|\n",
      "|  e/b38|924960|\n",
      "|  e/95b|124107|\n",
      "|  e/b42|241988|\n",
      "|  e/b3f|216993|\n",
      "|  e/b49|149398|\n",
      "|  e/b1c|  3325|\n",
      "|  e/b37|330085|\n",
      "|  e/b40|119173|\n",
      "|  s/af0|108146|\n",
      "|  e/b30|316395|\n",
      "|  e/b43|268391|\n",
      "|  e/b35|452098|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.groupBy(\"base_id\")\\\n",
    "    .count()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple aggregate, we need to use Spark Functions and add *. Since Agg accept column expression we can define all column in list and we can use *list syntax to unpack and send as each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|base_id|Latest_Pickup_Date|Latest_Dropoff_Date|\n",
      "+-------+------------------+-------------------+\n",
      "|  e/9d0|        2021-01-31|         2021-02-01|\n",
      "|  e/b3b|        2021-01-31|         2021-02-01|\n",
      "|  s/b13|        2021-01-31|         2021-02-01|\n",
      "|  e/a7a|        2021-01-31|         2021-02-01|\n",
      "|  s/b3d|        2021-01-31|         2021-02-01|\n",
      "|  s/b44|        2021-01-31|         2021-02-01|\n",
      "|  e/b3e|        2021-01-31|         2021-02-01|\n",
      "|  e/b48|        2021-01-31|         2021-02-01|\n",
      "|  e/b38|        2021-01-31|         2021-02-01|\n",
      "|  e/95b|        2021-01-31|         2021-02-01|\n",
      "|  e/b42|        2021-01-31|         2021-02-01|\n",
      "|  e/b3f|        2021-01-31|         2021-02-01|\n",
      "|  e/b49|        2021-01-31|         2021-02-01|\n",
      "|  e/b1c|        2021-01-31|         2021-01-31|\n",
      "|  e/b37|        2021-01-31|         2021-02-01|\n",
      "|  e/b40|        2021-01-31|         2021-02-01|\n",
      "|  s/af0|        2021-01-31|         2021-01-31|\n",
      "|  e/b30|        2021-01-31|         2021-02-01|\n",
      "|  e/b43|        2021-01-31|         2021-02-01|\n",
      "|  e/b35|        2021-01-31|         2021-02-01|\n",
      "+-------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df\\\n",
    "        .where((F.col(\"PULocationID\")>200) & (F.col(\"DOLocationID\")>200) )\\\n",
    "        .groupBy(\"base_id\")\\\n",
    "        .agg(*[F.max(\"pickup_date\").alias(\"Latest_Pickup_Date\"),\n",
    "          F.max(\"dropoff_date\").alias(\"Latest_Dropoff_Date\")])\\\n",
    "        .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
